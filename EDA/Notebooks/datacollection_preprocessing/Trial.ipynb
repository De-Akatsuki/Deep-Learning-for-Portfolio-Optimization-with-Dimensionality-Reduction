{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- At this stage we add calculated fields to the original fundamentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ticker",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "High",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Low",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Open",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Volume",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "530ea0a3-9e01-4dc4-b0f0-d21f2f7512df",
       "rows": [
        [
         "0",
         "SPY   ",
         " 2005-04-11",
         "81.12184906",
         "81.34854361",
         "80.94324599634402",
         "81.25924208",
         "44945000.0"
        ],
        [
         "1",
         "SPY   ",
         " 2005-04-12",
         "81.54089355",
         "81.78819576",
         "80.42116789375642",
         "80.98446620504191",
         "86144800.0"
        ],
        [
         "2",
         "SPY   ",
         " 2005-04-13",
         "80.57911682128906",
         "81.60954028324414",
         "80.46233151243722",
         "81.44466875580791",
         "65949000.0"
        ],
        [
         "3",
         "SPY   ",
         " 2005-04-14",
         "79.52814483642578",
         "80.71656983018221",
         "79.52814483642578",
         "80.64787592533705",
         "96119800.0"
        ],
        [
         "4",
         "SPY   ",
         " 2005-04-15",
         "78.41526794433594",
         "79.82351094197476",
         "78.38091838104894",
         "79.50751488",
         "128677300.0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2005-04-11</td>\n",
       "      <td>81.121849</td>\n",
       "      <td>81.348544</td>\n",
       "      <td>80.943246</td>\n",
       "      <td>81.259242</td>\n",
       "      <td>44945000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2005-04-12</td>\n",
       "      <td>81.540894</td>\n",
       "      <td>81.788196</td>\n",
       "      <td>80.421168</td>\n",
       "      <td>80.984466</td>\n",
       "      <td>86144800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2005-04-13</td>\n",
       "      <td>80.579117</td>\n",
       "      <td>81.609540</td>\n",
       "      <td>80.462332</td>\n",
       "      <td>81.444669</td>\n",
       "      <td>65949000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2005-04-14</td>\n",
       "      <td>79.528145</td>\n",
       "      <td>80.716570</td>\n",
       "      <td>79.528145</td>\n",
       "      <td>80.647876</td>\n",
       "      <td>96119800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2005-04-15</td>\n",
       "      <td>78.415268</td>\n",
       "      <td>79.823511</td>\n",
       "      <td>78.380918</td>\n",
       "      <td>79.507515</td>\n",
       "      <td>128677300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ticker         Date      Close       High        Low       Open  \\\n",
       "0  SPY      2005-04-11  81.121849  81.348544  80.943246  81.259242   \n",
       "1  SPY      2005-04-12  81.540894  81.788196  80.421168  80.984466   \n",
       "2  SPY      2005-04-13  80.579117  81.609540  80.462332  81.444669   \n",
       "3  SPY      2005-04-14  79.528145  80.716570  79.528145  80.647876   \n",
       "4  SPY      2005-04-15  78.415268  79.823511  78.380918  79.507515   \n",
       "\n",
       "        Volume  \n",
       "0   44945000.0  \n",
       "1   86144800.0  \n",
       "2   65949000.0  \n",
       "3   96119800.0  \n",
       "4  128677300.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = \"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/ETFs_datasets/Final_csv/merged_etfs.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Ticker', 'Date', 'Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54691 entries, 0 to 54690\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Ticker  54691 non-null  object \n",
      " 1   Date    54691 non-null  object \n",
      " 2   Close   54680 non-null  float64\n",
      " 3   High    54680 non-null  float64\n",
      " 4   Low     54680 non-null  float64\n",
      " 5   Open    54680 non-null  float64\n",
      " 6   Volume  54680 non-null  float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: index 13 is out of bounds for axis 0 with size 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4r/xd1m46rs2r1gx3mmpw3z84pw0000gn/T/ipykernel_51738/331979028.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb = df.groupby('Ticker', group_keys=False).apply(\n",
      "/var/folders/4r/xd1m46rs2r1gx3mmpw3z84pw0000gn/T/ipykernel_51738/331979028.py:38: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  macd = df.groupby('Ticker', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ta.trend import MACD, SMAIndicator, EMAIndicator, ADXIndicator\n",
    "from ta.volatility import AverageTrueRange, BollingerBands\n",
    "from ta.momentum import RSIIndicator\n",
    "from technical_indicator.volume import OnBalanceVolume\n",
    "import os\n",
    "\n",
    "\n",
    "def calculate_indicators_with_moving_averages(file_path):\n",
    "    # Load data\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found at {file_path}\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Validate required columns\n",
    "    required_cols = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns: {', '.join(missing_cols)}\")\n",
    "\n",
    "    df.sort_values(by=['Ticker', 'Date'] if 'Date' in df.columns else ['Ticker'], inplace=True)\n",
    "    df['returns'] = df.groupby('Ticker')['Close'].transform(\n",
    "    lambda x: x.pct_change() if len(x) >= 252 else np.nan\n",
    ")\n",
    "\n",
    "    # Trend Indicators\n",
    "    df['SMA_20'] = df.groupby('Ticker')['Close'].transform(lambda x: SMAIndicator(close=x, window=20).sma_indicator())\n",
    "    df['EMA_20'] = df.groupby('Ticker')['Close'].transform(lambda x: EMAIndicator(close=x, window=20).ema_indicator())\n",
    "\n",
    "    bb = df.groupby('Ticker', group_keys=False).apply(\n",
    "    lambda x: BollingerBands(close=x['Close'], window=20).bollinger_hband()\n",
    ")\n",
    "    df['Bollinger_High'] = bb.values\n",
    "\n",
    "    # Momentum Indicators\n",
    "    macd = df.groupby('Ticker', group_keys=False).apply(\n",
    "    lambda x: MACD(close=x['Close'], window_slow=26, window_fast=12, window_sign=9).macd()\n",
    ")\n",
    "    df['MACD'] = macd.values\n",
    "\n",
    "\n",
    "    # Volatility\n",
    "    df['ATR_14'] = df.groupby('Ticker').apply(lambda x: AverageTrueRange(x['High'], x['Low'], x['Close'], window=14).average_true_range()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Volume\n",
    "    df['OBV'] = df.groupby('Ticker').apply(lambda x: OnBalanceVolume(x['Close'], x['Volume']).calculate()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Oscillator\n",
    "    df['RSI_14'] = df.groupby('Ticker')['Close'].transform(lambda x: RSIIndicator(close=x, window=14).rsi())\n",
    "\n",
    "    # Risk-free rate\n",
    "    risk_free_rate = 0.02 / 252\n",
    "\n",
    "    df['Sharpe_Ratio'] = df.groupby('Ticker')['returns'].transform(lambda x: x.rolling(21).apply(lambda r: (r.mean() - risk_free_rate) / r.std() if r.std() else np.nan))\n",
    "    df['Sortino_Ratio'] = df.groupby('Ticker')['returns'].transform(lambda x: x.rolling(21).apply(lambda r: (r.mean() - risk_free_rate) / r[r < 0].std() if r[r < 0].std() else np.nan))\n",
    "\n",
    "    cumulative = df.groupby('Ticker')['returns'].transform(lambda x: (1 + x).cumprod())\n",
    "    peak = cumulative.groupby(df['Ticker']).cummax()\n",
    "    drawdown = (cumulative - peak) / peak\n",
    "    df['Max_Drawdown'] = drawdown.groupby(df['Ticker']).transform(lambda x: x.rolling(21).min())\n",
    "\n",
    "    df['Calmar_Ratio'] = df['returns'].groupby(df['Ticker']).transform(lambda x: x.rolling(21).mean()) / df['Max_Drawdown'].abs()\n",
    "    df['Volatility'] = df.groupby('Ticker')['returns'].transform(lambda x: x.rolling(21).std() * np.sqrt(252))\n",
    "    df['Cumulative_Return'] = df.groupby('Ticker')['returns'].transform(lambda x: (1 + x).cumprod() - 1)\n",
    "\n",
    "    df['Ticker_returns'] = df.groupby('Ticker')['Close'].pct_change()\n",
    "    df['Covariance'] = df.groupby('Ticker').apply(lambda x: x['returns'].rolling(21).cov(x['Ticker_returns'])).reset_index(level=0, drop=True)\n",
    "    df['Beta'] = df.groupby('Ticker').apply(lambda x: x['Covariance'] / x['Ticker_returns'].rolling(21).var()).reset_index(level=0, drop=True)\n",
    "    df['Alpha'] = df.groupby('Ticker').apply(lambda x: (x['returns'].rolling(21).mean() - risk_free_rate) - x['Beta'] * (x['Ticker_returns'].rolling(21).mean() - risk_free_rate)).reset_index(level=0, drop=True)\n",
    "\n",
    "    df['CAGR'] = df.groupby('Ticker')['Close'].transform(lambda x: (x / x.shift(252)) ** (1 / 1) - 1 if len(x) >= 252 else np.nan)\n",
    "    df['Total_Return'] = df.groupby('Ticker')['Close'].transform(lambda x: (x - x.shift(252)) / x.shift(252))\n",
    "\n",
    "    df['VaR_95'] = df.groupby('Ticker')['returns'].transform(lambda x: x.rolling(252).quantile(0.05))\n",
    "    df['CVaR_95'] = df.groupby('Ticker')['returns'].transform(lambda x: x.rolling(252).apply(lambda r: r[r <= r.quantile(0.05)].mean() if not r[r <= r.quantile(0.05)].empty else np.nan))\n",
    "    df['Drawdown_Duration'] = df['Max_Drawdown'].lt(0).groupby(df['Ticker']).transform(lambda x: x.rolling(252).sum())\n",
    "\n",
    "    adx = df.groupby('Ticker').apply(lambda x: pd.DataFrame({\n",
    "        'ADX': ADXIndicator(x['High'], x['Low'], x['Close'], window=14).adx(),\n",
    "        'DMI_plus': ADXIndicator(x['High'], x['Low'], x['Close'], window=14).adx_pos(),\n",
    "        'DMI_minus': ADXIndicator(x['High'], x['Low'], x['Close'], window=14).adx_neg()\n",
    "    }, index=x.index)).reset_index(level=0, drop=True)\n",
    "    df = df.join(adx)\n",
    "\n",
    "    # Ensure rolling window size is valid for each group\n",
    "    df['avg_daily_volume'] = df.groupby('Ticker')['Volume'].transform(\n",
    "    lambda x: x.rolling(252).mean() if len(x) >= 252 else np.nan\n",
    ")\n",
    "    df['correlation_with_benchmark'] = df.groupby('Ticker').apply(lambda x: x['returns'].rolling(252).corr(x['Ticker_returns'])).reset_index(level=0, drop=True)\n",
    "\n",
    "    for period in [10, 30, 50, 100, 200]:\n",
    "        df[f\"SMA_{period}\"] = df.groupby('Ticker')['Close'].transform(lambda x: SMAIndicator(close=x, window=period).sma_indicator())\n",
    "        df[f\"EMA_{period}\"] = df.groupby('Ticker')['Close'].transform(lambda x: EMAIndicator(close=x, window=period).ema_indicator())\n",
    "\n",
    "    df['Skewness'] = df.groupby('Ticker')['returns'].transform(lambda x: x.rolling(252).apply(pd.Series.skew))\n",
    "    df['Kurtosis'] = df.groupby('Ticker')['returns'].transform(lambda x: x.rolling(252).apply(pd.Series.kurt))\n",
    "\n",
    "    df['Win_Rate'] = df.groupby('Ticker')['returns'].transform(lambda x: (x > 0).rolling(252).mean())\n",
    "    df['Avg_Gain'] = df.groupby('Ticker')['returns'].transform(lambda x: x.where(x > 0).rolling(252).mean())\n",
    "    df['Avg_Loss'] = df.groupby('Ticker')['returns'].transform(lambda x: x.where(x < 0).rolling(252).mean())\n",
    "\n",
    "    df['Up_Capture_Ratio'] = df.groupby('Ticker').apply(lambda x: x['returns'].rolling(252).mean() / x['Ticker_returns'].rolling(252).mean()).reset_index(level=0, drop=True)\n",
    "    df['Down_Capture_Ratio'] = df.groupby('Ticker').apply(lambda x: x['returns'].where(x['returns'] < 0).rolling(252).mean() / x['Ticker_returns'].where(x['Ticker_returns'] < 0).rolling(252).mean()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Ulcer Index\n",
    "    def ulcer_index(series):\n",
    "        max_close = series.cummax()\n",
    "        drawdown = (series - max_close) / max_close\n",
    "        return np.sqrt((drawdown ** 2).mean())\n",
    "\n",
    "    df['Ulcer_Index'] = df.groupby('Ticker')['Close'].transform(lambda x: x.rolling(14).apply(ulcer_index))\n",
    "\n",
    "    # Rate of Change (ROC)\n",
    "    df['ROC_14'] = df.groupby('Ticker')['Close'].transform(lambda x: x.pct_change(periods=14))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/ETFs_datasets/Final_csv/merged_etfs.csv\"\n",
    "    output_path = \"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/ETFs_datasets/Final_csv/indicators_output_new.csv\"\n",
    "\n",
    "    try:\n",
    "        indicators_df = calculate_indicators_with_moving_averages(file_path)\n",
    "        indicators_df.to_csv(output_path, index=False)\n",
    "        print(\"Indicators calculated and saved to:\", output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Date column:\n",
      "['11/04/2005' '12/04/2005' '13/04/2005' ... '07/04/2025' '08/04/2025'\n",
      " '09/04/2025']\n",
      "'Date' column successfully converted to datetime objects.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     71\u001b[39m target = target.fillna(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# Or target.dropna()\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# # Scale the features\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# scaler = StandardScaler()\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# features_scaled = scaler.fit_transform(features)\u001b[39;00m\n\u001b[32m     76\u001b[39m \n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# the train_size is 70% and the test_size is 30%\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m X_train, X_test, y_train, y_test, metadata_train, metadata_test = train_test_split(\n\u001b[32m     80\u001b[39m     features, target, test_size=\u001b[32m0.3\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Retain 'ticker' and 'Date' columns for reference\u001b[39;00m\n\u001b[32m     83\u001b[39m metadata = data[[\u001b[33m'\u001b[39m\u001b[33mTicker\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "file_path = ('/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/EDA/Notebooks/datacollection_preprocessing/cleaned_financial_data_final.csv') # Replace with your actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Strip whitespace from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Inspect unique values in the 'Date' column to identify potential formatting issues\n",
    "print(\"Unique values in Date column:\")\n",
    "print(data['Date'].unique())\n",
    "\n",
    "# Function to attempt parsing with different date formats\n",
    "def parse_date(date_string):\n",
    "    formats = [\"%Y-%m-%d\", \"%m/%d/%Y\", \"%d/%m/%Y\", \"%Y%m%d\"]  # Add more formats if needed\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_string, format=fmt)\n",
    "        except ValueError:\n",
    "            continue  # Try the next format if this one fails\n",
    "    return pd.NaT  # Return NaT if none of the formats work\n",
    "\n",
    "# Apply the parsing function to the 'Date' column\n",
    "data['Date'] = data['Date'].astype(str).str.strip()  # Ensure it's string and strip whitespace\n",
    "data['Date'] = data['Date'].apply(parse_date)\n",
    "\n",
    "# Drop rows where 'Date' could not be parsed\n",
    "data = data.dropna(subset=['Date'])\n",
    "\n",
    "# Check if 'Date' column is successfully converted to datetime objects\n",
    "if pd.api.types.is_datetime64_any_dtype(data['Date']):\n",
    "    print(\"'Date' column successfully converted to datetime objects.\")\n",
    "else:\n",
    "    print(\"Failed to convert 'Date' column to datetime objects.\")\n",
    "\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# Adjust column names based on your dataset\n",
    "features = data[[\n",
    "\"Date\", \"Ticker\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\", \"returns\", \n",
    "    \"SMA_10\", \"EMA_10\", \"SMA_20\", \"EMA_20\", \"SMA_30\", \"EMA_30\", \"SMA_50\", \"EMA_50\", \n",
    "    \"SMA_100\", \"EMA_100\", \"SMA_200\", \"EMA_200\", \n",
    "    \"BB_Upper_20\", \"BB_Middle_20\", \"BB_Lower_20\", \n",
    "    \"BB_Upper_50\", \"BB_Middle_50\", \"BB_Lower_50\", \n",
    "    \"RSI_14\", \"RSI_30\", \"MACD\", \"MACD_Signal\", \n",
    "    \"ADX_14\", \"DMI_plus_14\", \"DMI_minus_14\", \n",
    "    \"ADX_30\", \"DMI_plus_30\", \"DMI_minus_30\", \n",
    "    \"ATR_14\", \"ATR_30\", \"Stoch_K\", \"Stoch_D\", \"OBV\", \n",
    "    \"Beta\", \"Alpha\", \"Avg_Daily_Volume\", \"Skewness\", \"Kurtosis\", \"Win_Rate\", \n",
    "    \"Up_Capture_Ratio\", \"cpi\", \"core_cpi\", \"pce_price_index\", \"core_pce_price_index\", \n",
    "    \"import_price_index\", \"export_price_index\", \"real_gdp\", \"industrial_production\", \n",
    "    \"capacity_utilization\", \"durable_goods_orders\", \"real_pce\", \"real_private_investment\", \n",
    "    \"net_exports\", \"govt_expenditures\", \"unemployment_rate\", \"labor_force_participation\", \n",
    "    \"jobless_claims\", \"nonfarm_payrolls\", \"average_hourly_earnings\", \"job_openings\", \n",
    "    \"quits_rate\", \"labor_productivity\", \"fed_funds_rate\", \"treasury_10y\", \n",
    "    \"treasury_3m\", \"baa_yield\", \"aaa_yield\", \"consumer_credit\", \"money_supply_m2\", \n",
    "    \"mortgage_rate_30y\", \"bank_prime_rate\", \"credit_card_rate\", \n",
    "    \"leading_economic_index\", \"weekly_economic_index\", \"housing_starts\", \n",
    "    \"building_permits\", \"new_home_sales\", \"home_price_index\", \n",
    "    \"rental_vacancy_rate\", \"umich_consumer_sentiment\"\n",
    "]]\n",
    "target = data['returns']\n",
    "\n",
    "# Handle missing values by filling with 0 or dropping\n",
    "features = features.fillna(0)  # Or features.dropna()\n",
    "target = target.fillna(0)  # Or target.dropna()\n",
    "\n",
    "# # Scale the features\n",
    "# scaler = StandardScaler()\n",
    "# features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# the train_size is 70% and the test_size is 30%\n",
    "X_train, X_test, y_train, y_test, metadata_train, metadata_test = train_test_split(\n",
    "    features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Retain 'ticker' and 'Date' columns for reference\n",
    "metadata = data[['Ticker', 'Date']]\n",
    "\n",
    "# Split the metadata into training and testing sets (same split as features and target)\n",
    "# metadata_train, metadata_test = train_test_split(\n",
    "#     metadata, test_size=0.30, random_state=42\n",
    "# )\n",
    "\n",
    "metadata = data.loc[features.index, ['Ticker', 'Date']]\n",
    "metadata_train, metadata_test = train_test_split(\n",
    "    metadata, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# # Convert scaled features back to DataFrame with original column names\n",
    "# X_train = pd.DataFrame(X_train, columns=features.columns)\n",
    "# X_test = pd.DataFrame(X_test, columns=features.columns)\n",
    "\n",
    "base_dir = \"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/EDA/ETFs_datasets/Model Data/Pre-Dim Reduction\"\n",
    "train_dir = os.path.join(base_dir, \"Training_Dataset_1\")\n",
    "test_dir = os.path.join(base_dir, \"Test_Dataset_1\")\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Save features and targets\n",
    "X_train.to_csv(os.path.join(train_dir, \"X_train.csv\"), index=False)\n",
    "y_train.to_csv(os.path.join(train_dir, \"y_train.csv\"), index=False)\n",
    "metadata_train.to_csv(os.path.join(train_dir, \"metadata_train.csv\"), index=False)\n",
    "\n",
    "X_test.to_csv(os.path.join(test_dir, \"X_test.csv\"), index=False)\n",
    "y_test.to_csv(os.path.join(test_dir, \"y_test.csv\"), index=False)\n",
    "metadata_test.to_csv(os.path.join(test_dir, \"metadata_test.csv\"), index=False)\n",
    "\n",
    "print(\"✅ All files saved successfully.\")\n",
    "\n",
    "# # Save metadata to CSV files\n",
    "# metadata_train_file = os.path.join(\"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/EDA/ETFs_datasets/Model Data/Pre-Dim Reduction/Training_Dataset\")\n",
    "# metadata_test_file = os.path.join(\"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/EDA/ETFs_datasets/Model Data/Pre-Dim Reduction/Test_Dataset\")\n",
    "\n",
    "# metadata_train.to_csv(metadata_train_file, index=False)\n",
    "# metadata_test.to_csv(metadata_test_file, index=False)\n",
    "\n",
    "# # Save training and testing sets to CSV files in the specified folder\n",
    "# train_features_file = os.path.join(\"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/ETFs/Model Data/EDA/ETFs_datasets/Model Data/Pre-Dim Reduction/Test_Dataset/X_train1.csv\")\n",
    "# test_features_file = os.path.join(\"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/ETFs/Model Data/EDA/ETFs_datasets/Model Data/Pre-Dim Reduction/Training_Dataset/X_test1.csv\")\n",
    "# train_target_file = os.path.join(\"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/ETFs/Model Data/EDA/ETFs_datasets/Model Data/Pre-Dim Reduction/Training_Dataset/y_train1.csv\")\n",
    "# test_target_file = os.path.join(\"/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/ETFs/Model Data/EDA/ETFs_datasets/Model Data/Pre-Dim Reduction/Test_Dataset/y_test1.csv\")\n",
    "\n",
    "# X_train.to_csv(train_features_file, index=False)\n",
    "# X_test.to_csv(test_features_file, index=False)\n",
    "# y_train.to_csv(train_target_file, index=False)\n",
    "# y_test.to_csv(test_target_file, index=False)\n",
    "\n",
    "# # Print confirmation messages\n",
    "# print(f\"Training features saved to {train_features_file}\")\n",
    "# print(f\"Testing features saved to {test_features_file}\")\n",
    "# print(f\"Training target saved to {train_target_file}\")\n",
    "# print(f\"Testing target saved to {test_target_file}\")\n",
    "# print(f\"Metadata for training saved to {metadata_train_file}\")\n",
    "# print(f\"Metadata for testing saved to {metadata_test_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([32, 2])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([16, 2])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/.venv/lib/python3.13/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([13, 2])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 389072850068408.7500 | Val Loss: 271315050.9959\n",
      "Epoch 2 | Train Loss: 1138782121050.0867 | Val Loss: 0.0022\n",
      "Epoch 3 | Train Loss: 328302712268.2966 | Val Loss: 0.0022\n",
      "Epoch 4 | Train Loss: 183141171822.6465 | Val Loss: 0.0023\n",
      "Epoch 5 | Train Loss: 112242877664.9248 | Val Loss: 0.0025\n",
      "Epoch 6 | Train Loss: 110321585718.8709 | Val Loss: 0.0029\n",
      "Epoch 7 | Train Loss: 69237073612.4652 | Val Loss: 0.0022\n",
      "Epoch 8 | Train Loss: 22740954108.6334 | Val Loss: 0.0017\n",
      "Epoch 9 | Train Loss: 10663651186.1988 | Val Loss: 0.0026\n",
      "Epoch 10 | Train Loss: 25118837314.4111 | Val Loss: 0.0022\n",
      "Epoch 11 | Train Loss: 11362162884.9065 | Val Loss: 0.0007\n",
      "Epoch 12 | Train Loss: 16913912229.8732 | Val Loss: 0.0002\n",
      "Epoch 13 | Train Loss: 1708864543.2861 | Val Loss: 0.0004\n",
      "Epoch 14 | Train Loss: 26179648.6781 | Val Loss: 0.0004\n",
      "Epoch 15 | Train Loss: 151830491.4661 | Val Loss: 0.0006\n",
      "Epoch 16 | Train Loss: 100531828.6248 | Val Loss: 0.0020\n",
      "Epoch 17 | Train Loss: 2295195040.6416 | Val Loss: 0.0078\n",
      "Epoch 18 | Train Loss: 832376498.6304 | Val Loss: 0.0082\n",
      "Epoch 19 | Train Loss: 77001399.5515 | Val Loss: 0.0092\n",
      "Epoch 20 | Train Loss: 32485754751.8212 | Val Loss: 0.0060\n",
      "Epoch 21 | Train Loss: 2126429691.3137 | Val Loss: 0.0072\n",
      "Epoch 22 | Train Loss: 190941142.8369 | Val Loss: 0.0058\n",
      "⏹️ Early stopping triggered.\n",
      "\n",
      "🕒 Training Time: 31.32 seconds\n",
      "💾 Peak Memory Usage: 1.27 MB\n",
      "🧠 CPU Usage Delta: 51.20%\n",
      "✅ Best model saved to: /Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/EDA/Notebooks/deep_learning_models/Without_DR/MLP1.pth\n",
      "✅ Predictions saved to mlp_predictions1.csv\n",
      "MSE: 0.0059, MAE: 0.0756, R²: -21.9991\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "import tracemalloc\n",
    "import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ---------- CONFIGURATION ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "base_dir = '/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/EDA'\n",
    "directory = os.path.join(base_dir, \"ETFs_datasets/Model Data/Pre-Dim Reduction\")\n",
    "train_data_path = os.path.join(directory, \"Training_Dataset\")\n",
    "test_data_path = os.path.join(directory, \"Test_Dataset\")\n",
    "# ------------------------------------\n",
    "\n",
    "# Step 1: Load data (preserve all columns)\n",
    "def load_csv(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "X_train = load_csv(f\"{train_data_path}/X_train_new.csv\")\n",
    "y_train = load_csv(f\"{train_data_path}/y_train_new.csv\")\n",
    "X_test = load_csv(f\"{test_data_path}/X_test_new.csv\")\n",
    "y_test = load_csv(f\"{test_data_path}/y_test_new.csv\")\n",
    "\n",
    "# Step 2: Separate Date column if present\n",
    "date_col = None\n",
    "for col in X_train.columns:\n",
    "    if 'date' in col.lower():\n",
    "        date_col = col\n",
    "        break\n",
    "\n",
    "if date_col:\n",
    "    X_train_dates = X_train[date_col]\n",
    "    X_test_dates = X_test[date_col]\n",
    "    # Drop date column for modeling\n",
    "    X_train = X_train.drop(columns=[date_col])\n",
    "    X_test = X_test.drop(columns=[date_col])\n",
    "else:\n",
    "    X_train_dates = None\n",
    "    X_test_dates = None\n",
    "\n",
    "# Step 3: Ensure numeric (object to numeric, handle missing values)\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "y_train = y_train.apply(pd.to_numeric, errors='coerce')\n",
    "y_test = y_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill or drop NaNs as appropriate (here, fill with 0)\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "y_train = y_train.fillna(0)\n",
    "y_test = y_test.fillna(0)\n",
    "\n",
    "# Step 4: Create validation split from training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Step 5: Convert to PyTorch tensors and DataLoader\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Step 6: Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = MLP(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 7: Train model with early stopping and track metrics\n",
    "epochs = 100\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = model.state_dict()\n",
    "\n",
    "tracemalloc.start()\n",
    "train_start_time = time.time()\n",
    "cpu_start = psutil.cpu_percent(interval=None)\n",
    "memory_start = psutil.virtual_memory().used\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            val_preds = model(X_val_batch)\n",
    "            val_loss = criterion(val_preds, y_val_batch)\n",
    "            val_losses.append(val_loss.item())\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"⏹️ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Step 8: Metrics after training\n",
    "train_end_time = time.time()\n",
    "current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
    "cpu_end = psutil.cpu_percent(interval=None)\n",
    "memory_end = psutil.virtual_memory().used\n",
    "tracemalloc.stop()\n",
    "\n",
    "training_time = train_end_time - train_start_time\n",
    "memory_usage_mb = peak_mem / (1024 ** 2)\n",
    "cpu_usage = cpu_end - cpu_start\n",
    "\n",
    "print(f\"\\n🕒 Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"💾 Peak Memory Usage: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"🧠 CPU Usage Delta: {cpu_usage:.2f}%\")\n",
    "\n",
    "# Step 9: Save best model\n",
    "model.load_state_dict(best_model_state)\n",
    "output_model_path = os.path.join(base_dir,\"Notebooks/deep_learning_models/Without_DR/MLP1.pth\")\n",
    "os.makedirs(os.path.dirname(output_model_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), output_model_path)\n",
    "print(f\"✅ Best model saved to: {output_model_path}\")\n",
    "\n",
    "# Step 10: Generate predictions and save to CSV (with length alignment)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    y_true = y_test_tensor.cpu().numpy().flatten()\n",
    "\n",
    "# Align columns before saving to CSV\n",
    "if X_test_dates is not None:\n",
    "    X_test_dates_aligned = pd.Series(X_test_dates).reset_index(drop=True)\n",
    "    y_true_aligned = pd.Series(y_true).reset_index(drop=True)\n",
    "    y_pred_aligned = pd.Series(y_pred).reset_index(drop=True)\n",
    "    min_len = min(len(X_test_dates_aligned), len(y_true_aligned), len(y_pred_aligned))\n",
    "    results = pd.DataFrame({\n",
    "        'Date': X_test_dates_aligned[:min_len],\n",
    "        'Actual': y_true_aligned[:min_len],\n",
    "        'Predicted': y_pred_aligned[:min_len]\n",
    "    })\n",
    "else:\n",
    "    results = pd.DataFrame({'Actual': y_true, 'Predicted': y_pred})\n",
    "\n",
    "results.to_csv('mlp_predictions.csv', index=False)\n",
    "print(\"✅ Predictions saved to mlp_predictions1.csv\")\n",
    "\n",
    "# Step 11: Evaluate predictive performance\n",
    "mse = mean_squared_error(results['Actual'], results['Predicted'])\n",
    "mae = mean_absolute_error(results['Actual'], results['Predicted'])\n",
    "r2 = r2_score(results['Actual'], results['Predicted'])\n",
    "print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
