{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid: Transformer + Bi-LSTM (Autoencoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import psutil\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "\n",
    "# --- 1. Data loading and preparation ---\n",
    "data = pd.read_csv('/Users/imperator/Documents/STUDIES/UNIVERSITY OF GHANA/RESEARCH WORK/CORCHIL KELLY KWAME/PORTFOLIO OPTIMIZATION/PROJECT CODE/Dimensionality-Reduction-PortfolioOptimization/EDA/ETFs_datasets/Final_csv/5-cleaned_etf_data.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'], dayfirst=True, errors='coerce')\n",
    "data = data.dropna(subset=['Date'])\n",
    "data = data.sort_values(['Date', 'Ticker']).reset_index(drop=True)\n",
    "\n",
    "assets = ['ARKK', 'AGG', 'GLD', 'HYG', 'SPY', 'USO', 'VNQ', 'VXUS']\n",
    "features = ['returns','RSI_14','Volatility_21','MACD','Beta','Alpha','Volume','DMI_minus_14','DMI_plus_14',\n",
    "           'Calmar_Ratio_21','ATR_14','Open','Close','High','nonfarm_payrolls','real_gdp','cpi','treasury_10y','building_permits']\n",
    "\n",
    "df_wide = pd.DataFrame()\n",
    "for feat in features:\n",
    "    temp = data.pivot(index='Date', columns='Ticker', values=feat)\n",
    "    temp.columns = [f\"{col}_{feat}\" for col in temp.columns]\n",
    "    df_wide = pd.concat([df_wide, temp], axis=1) if not df_wide.empty else temp\n",
    "df_wide = df_wide.dropna()\n",
    "\n",
    "returns_cols = [f\"{asset}_returns\" for asset in assets]\n",
    "etf_returns = df_wide[returns_cols]\n",
    "\n",
    "feature_cols = []\n",
    "for asset in assets:\n",
    "    for feat in features:\n",
    "        if feat != 'returns':\n",
    "            col = f\"{asset}_{feat}\"\n",
    "            if col in df_wide.columns:\n",
    "                feature_cols.append(col)\n",
    "\n",
    "features_data = df_wide[feature_cols]\n",
    "\n",
    "# --- StandardScaler ---\n",
    "scaler_features = StandardScaler()\n",
    "features_scaled = scaler_features.fit_transform(features_data.values)\n",
    "\n",
    "# --- Autoencoder definition ---\n",
    "latent_dim = 7\n",
    "input_dim = features_scaled.shape[1]\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, z\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = Autoencoder(input_dim, latent_dim).to(device)\n",
    "features_tensor = torch.tensor(features_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "class FeaturesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "train_dataset = FeaturesDataset(features_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Train autoencoder\n",
    "autoencoder.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, _ = autoencoder(batch_data)\n",
    "        loss = loss_fn(reconstructed, batch_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.6f}\")\n",
    "\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    _, features_latent = autoencoder(features_tensor)\n",
    "features_latent_np = features_latent.cpu().numpy()\n",
    "\n",
    "window_size = 30\n",
    "\n",
    "def risk_parity_weights(returns_window):\n",
    "    cov = np.cov(returns_window.T)\n",
    "    vol = np.sqrt(np.diag(cov))\n",
    "    inv_vol = 1.0 / (vol + 1e-8)\n",
    "    weights = inv_vol / inv_vol.sum()\n",
    "    return weights\n",
    "\n",
    "def generate_dynamic_weights_with_risk(returns_df, window=window_size):\n",
    "    weights_list = []\n",
    "    for i in range(len(returns_df) - 2*window):\n",
    "        train_w = returns_df.iloc[i:i+window].values\n",
    "        test_w = returns_df.iloc[i+window:i+2*window]\n",
    "        mean_returns = test_w.mean()\n",
    "        mean_returns[mean_returns < 0] = 0\n",
    "        r_weights = risk_parity_weights(train_w)\n",
    "        combined = mean_returns * r_weights\n",
    "        combined[combined < 0] = 0\n",
    "        if combined.sum() == 0:\n",
    "            weights = np.ones(len(mean_returns)) / len(mean_returns)\n",
    "        else:\n",
    "            weights = combined / combined.sum()\n",
    "        weights_list.append(weights)\n",
    "    return np.array(weights_list)\n",
    "\n",
    "dynamic_weights = generate_dynamic_weights_with_risk(etf_returns, window=window_size)\n",
    "\n",
    "def standardize_targets(y):\n",
    "    scaler = StandardScaler()\n",
    "    y_scaled = scaler.fit_transform(y.reshape(-1,1)).flatten()\n",
    "    return y_scaled, scaler\n",
    "\n",
    "def create_sequences(features_data, returns_data, weights_data, window, clip_sharpe=True):\n",
    "    X, y_sharpe, y_weights = [], [], []\n",
    "    limit = len(features_data) - 2*window\n",
    "    for i in range(limit):\n",
    "        seq_x = features_data[i:i+window]\n",
    "        returns_w = returns_data.iloc[i+window:i+2*window]\n",
    "        mean_ret = returns_w.mean().mean()\n",
    "        std_ret = returns_w.std().mean()\n",
    "        sharpe = mean_ret / (std_ret + 1e-6)\n",
    "        if clip_sharpe:\n",
    "            sharpe = np.clip(sharpe, -5, 5)\n",
    "        weight_target = weights_data[i] if i < len(weights_data) else np.ones(len(assets))/len(assets)\n",
    "        X.append(seq_x)\n",
    "        y_sharpe.append(sharpe)\n",
    "        y_weights.append(weight_target)\n",
    "    y_sharpe_arr = np.array(y_sharpe)\n",
    "    y_sharpe_scaled, scaler = standardize_targets(y_sharpe_arr)\n",
    "    return np.array(X), y_sharpe_scaled, np.array(y_weights), scaler\n",
    "\n",
    "X, y_sharpe_scaled, y_weights, sharpe_scaler = create_sequences(features_latent_np, etf_returns, dynamic_weights, window_size)\n",
    "\n",
    "class PortfolioDataset(Dataset):\n",
    "    def __init__(self, X, y_sharpe, y_weights):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_sharpe = torch.tensor(y_sharpe, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y_weights = torch.tensor(y_weights, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_sharpe[idx], self.y_weights[idx]\n",
    "\n",
    "X_train, X_test, y_sharpe_train, y_sharpe_test, y_weights_train, y_weights_test = train_test_split(\n",
    "    X, y_sharpe_scaled, y_weights, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = PortfolioDataset(X_train, y_sharpe_train, y_weights_train)\n",
    "test_dataset = PortfolioDataset(X_test, y_sharpe_test, y_weights_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].detach()\n",
    "        return x\n",
    "\n",
    "class HybridTransformerBiLSTM(nn.Module):\n",
    "    def __init__(self, pca_components, n_assets, d_model=64, nhead=8, num_transformer_layers=2, lstm_hidden=64):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(pca_components, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=256, dropout=0.1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_transformer_layers)\n",
    "        self.bilstm = nn.LSTM(input_size=d_model, hidden_size=lstm_hidden, batch_first=True, bidirectional=True)\n",
    "        self.fc_shared = nn.Linear(lstm_hidden * 2, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sharpe_out = nn.Linear(128, 1)\n",
    "        self.weights_out = nn.Linear(128, n_assets)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "        x = transformer_output.permute(1, 0, 2)\n",
    "        lstm_out, _ = self.bilstm(x)\n",
    "        last_step = lstm_out[:, -1, :]\n",
    "        feat = self.relu(self.fc_shared(last_step))\n",
    "        feat = self.dropout(feat)\n",
    "        sharpe = self.sharpe_out(feat)\n",
    "        weights = self.softmax(self.weights_out(feat))\n",
    "        return sharpe, weights\n",
    "\n",
    "model = HybridTransformerBiLSTM(\n",
    "    pca_components=latent_dim,\n",
    "    n_assets=len(assets),\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_transformer_layers=2,\n",
    "    lstm_hidden=64\n",
    ").to(device)\n",
    "\n",
    "criterion_sharpe = nn.MSELoss()\n",
    "criterion_weights = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 50\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "cpu_start = psutil.cpu_percent(interval=None)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_x, batch_sharpe, batch_weights in train_loader:\n",
    "        batch_x, batch_sharpe, batch_weights = batch_x.to(device), batch_sharpe.to(device), batch_weights.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_sharpe, pred_weights = model(batch_x)\n",
    "        loss_sharpe = criterion_sharpe(pred_sharpe, batch_sharpe)\n",
    "        loss_weights = criterion_weights(pred_weights, batch_weights)\n",
    "        loss = loss_sharpe * 0.1 + loss_weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.6f}\")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "cpu_end = psutil.cpu_percent(interval=None)\n",
    "cpu_usage = cpu_end - cpu_start\n",
    "\n",
    "model.eval()\n",
    "all_pred_sharpe = []\n",
    "all_true_sharpe = []\n",
    "all_pred_weights = []\n",
    "all_true_weights = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_sharpe, batch_weights in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        pred_sharpe, pred_weights = model(batch_x)\n",
    "        all_pred_sharpe.append(pred_sharpe.cpu().numpy())\n",
    "        all_pred_weights.append(pred_weights.cpu().numpy())\n",
    "        all_true_sharpe.append(batch_sharpe.cpu().numpy())\n",
    "        all_true_weights.append(batch_weights.cpu().numpy())\n",
    "\n",
    "pred_sharpe_scaled = np.vstack(all_pred_sharpe).flatten()\n",
    "true_sharpe_scaled = np.vstack(all_true_sharpe).flatten()\n",
    "pred_weights = np.vstack(all_pred_weights)\n",
    "pred_weights /= pred_weights.sum(axis=1, keepdims=True)\n",
    "true_weights = np.vstack(all_true_weights)\n",
    "\n",
    "pred_sharpe = sharpe_scaler.inverse_transform(pred_sharpe_scaled.reshape(-1,1)).flatten()\n",
    "true_sharpe = sharpe_scaler.inverse_transform(true_sharpe_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "test_returns = etf_returns.iloc[-len(pred_weights):].values\n",
    "portfolio_returns = np.sum(test_returns * pred_weights, axis=1)\n",
    "turnover = np.mean(np.sum(np.abs(np.diff(pred_weights, axis=0)), axis=1))\n",
    "\n",
    "def sortino_ratio(returns, risk_free=0, periods_per_year=252):\n",
    "    returns = pd.Series(returns)\n",
    "    downside = returns[returns < risk_free]\n",
    "    expected_return = returns.mean() * periods_per_year\n",
    "    downside_std = downside.std() * np.sqrt(periods_per_year)\n",
    "    return expected_return / downside_std if downside_std > 0 else np.nan\n",
    "\n",
    "def calmar_ratio(returns, period_per_year=252):\n",
    "    returns = pd.Series(returns)\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    annualized_return = cumulative.iloc[-1] ** (period_per_year / len(returns)) - 1\n",
    "    max_drawdown = ((cumulative.cummax() - cumulative) / cumulative.cummax()).max()\n",
    "    return annualized_return / max_drawdown if max_drawdown > 0 else np.nan\n",
    "\n",
    "mse_sharpe = mean_squared_error(true_sharpe, pred_sharpe)\n",
    "mae_sharpe = mean_absolute_error(true_sharpe, pred_sharpe)\n",
    "r2_sharpe = r2_score(true_sharpe, pred_sharpe)\n",
    "sortino = sortino_ratio(portfolio_returns)\n",
    "calmar = calmar_ratio(portfolio_returns)\n",
    "\n",
    "def evaluate_portfolio_performance(portfolio_returns):\n",
    "    portfolio_returns = pd.Series(portfolio_returns).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    periods_per_year = 252\n",
    "    portfolio_values = (1 + portfolio_returns).cumprod()\n",
    "    peak = portfolio_values.cummax()\n",
    "    drawdown = (peak - portfolio_values) / peak\n",
    "    max_drawdown = drawdown.max()\n",
    "    total_periods = len(portfolio_returns)\n",
    "    annualized_return = (portfolio_values.iloc[-1]) ** (periods_per_year / total_periods) - 1\n",
    "    annualized_volatility = portfolio_returns.std() * np.sqrt(periods_per_year)\n",
    "    mean_return = portfolio_returns.mean() * periods_per_year\n",
    "    sharpe_ratio = mean_return / annualized_volatility if annualized_volatility > 0 else np.nan\n",
    "    return {\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Annualized Return': annualized_return,\n",
    "        'Annualized Volatility': annualized_volatility,\n",
    "        'Max Drawdown': max_drawdown\n",
    "    }\n",
    "\n",
    "performance_metrics = evaluate_portfolio_performance(portfolio_returns)\n",
    "\n",
    "print(\"\\n--- Portfolio Performance with Hybrid Transformer + BiLSTM + Autoencoder ---\")\n",
    "print(f\"Training time (seconds): {train_time:.2f}\")\n",
    "print(f\"CPU usage change (%): {cpu_usage:.2f}\\n\")\n",
    "\n",
    "for metric, value in performance_metrics.items():\n",
    "    if \"Return\" in metric or \"Volatility\" in metric or \"Drawdown\" in metric:\n",
    "        print(f\"- {metric}: {value*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"- {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"- Sortino Ratio: {sortino:.4f}\")\n",
    "print(f\"- Calmar Ratio: {calmar:.4f}\")\n",
    "print(f\"- Average Turnover: {turnover:.4f}\")\n",
    "print(f\"- Sharpe Prediction MSE: {mse_sharpe:.6f}\")\n",
    "print(f\"- Sharpe Prediction MAE: {mae_sharpe:.6f}\")\n",
    "print(f\"- Sharpe Prediction R2: {r2_sharpe:.4f}\")\n",
    "\n",
    "# Save predicted portfolio weights\n",
    "weights_df = pd.DataFrame(pred_weights, index=etf_returns.index[-len(pred_weights):], columns=assets)\n",
    "weights_df.to_csv('hybrid_transformer_bilstm_autoencoder_portfolio_weights.csv')\n",
    "print(\"Saved predicted portfolio weights to 'hybrid_transformer_bilstm_autoencoder_portfolio_weights.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
